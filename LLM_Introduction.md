# 📌 LLM을 위한 교양 (TIL)

## 🔹 퍼셉트론 (Perceptron)
- 1958년 프랑크 로젠블랫(Frank Rosenblatt)이 개발한 **최초의 신경망 모델**.
- 입력값에 **가중치(weight)**와 **편향(bias)**을 적용하여 **이진 분류(binary classification)**를 수행하는 **기초적인 인공 신경망**.
- 수식:

  $$
  y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
  $$

  여기서 \( f \)는 활성화 함수(activation function)

### ❌ 한계점
- 선형적으로 구분되지 않는 데이터(ex. XOR 문제)를 해결할 수 없음.
- 다층 구조가 필요 → 이를 해결한 것이 **다중 퍼셉트론(MLP, Multi-Layer Perceptron)**.

---

## 🔹 트랜스포머 기술의 발전

### 1️⃣ MLP (다중 퍼셉트론, Multi-Layer Perceptron)
- 여러 개의 뉴런을 **다층(hidden layer) 구조**로 배치한 모델.
- 데이터를 기반으로 **숨겨진 패턴을 학습**하여 예측을 수행.
- 하지만 **순서(sequence)를 고려하는 학습이 어려움** → 시퀀스 데이터(문장, 음성 등)에 적합하지 않음.

### 2️⃣ RNN (순환 신경망, Recurrent Neural Network)
- 데이터를 **순차적으로 처리**하며 **이전 단계의 정보를 기억**할 수 있음.
- NLP 분야에서 문장, 음성 데이터 처리에 사용됨.
- ✅ **장점**: 시계열 및 자연어 데이터와 같은 **순서가 있는 데이터 처리 가능**.
- ❌ **문제점**:
- **장기 의존성 문제(Long-Term Dependency)** → 앞쪽 데이터가 뒤쪽으로 전달되면서 소실됨(기울기 소실 문제, Vanishing Gradient).
- **병렬 연산 불가** → 훈련 속도가 느림.

### 3️⃣ Transformer (트랜스포머 모델)
- 2017년 Google이 **"Attention Is All You Need"** 논문에서 제안한 모델.
- RNN의 단점을 극복하기 위해 **어텐션(attention) 메커니즘**을 활용하여 **병렬 연산** 가능.
- **핵심 개념**:
- **Self-Attention**: 문장 내 단어 간의 관계를 계산하여 중요한 정보를 집중적으로 학습.
- **Multi-Head Attention**: 다양한 각도에서 단어 관계를 분석하여 보다 정교한 결과 도출.
- **Position Encoding**: 순서 정보가 없는 트랜스포머 구조에서 단어의 위치 정보를 보완.
- ✅ **장점**:
- **순서를 고려한 병렬 연산 가능** → RNN보다 훨씬 빠르고 효율적.
- **장기 의존성 문제 해결** → 문장 전체의 맥락을 잘 파악.
- **대규모 학습 데이터 활용 가능** → GPT, BERT 등 대형 모델의 기반이 됨.

---

## 🔹 LLM (대규모 언어 모델, Large Language Model)
- **엄청난 양의 텍스트 데이터를 학습하여 단어 간 관계를 수치화한 모델**.
- 기존 데이터베이스(DB)처럼 저장된 정보를 검색하는 것이 아니라, **새로운 답변을 생성하는 생성형 모델**.

> 📌 **"ChatGPT는 데이터베이스가 아니라 추론 엔진이다."** — *샘 알트먼 (Sam Altman, OpenAI CEO)*

### ✅ LLM의 특징
1. **대량의 파라미터 (Parameters)**
 - 예: GPT-3 (175B), GPT-4 (1T+ 추정) → 모델이 커질수록 성능이 비약적으로 향상됨.
2. **확률적 생성 방식**
 - 단어를 예측하여 새로운 텍스트를 생성하는 **확률 모델**.
3. **워드 임베딩 (Word Embedding)**
 - 단어 간 의미적 관계를 벡터로 변환하여 수치화.
4. **사전 훈련 (Pretraining) + 미세 조정 (Fine-Tuning)**
 - 대량의 텍스트를 사전 학습한 후, 특정 도메인에 맞게 추가 훈련 가능.

---

## 🔹 프롬프트 엔지니어링 (Prompt Engineering)
- **프롬프팅 (Prompting)**: 질문을 최적화하여 원하는 답변을 얻는 기술.
- **파라미터 수가 증가할수록 프롬프트 엔지니어링의 중요성이 증가**.

### ✅ 효과적인 프롬프트 전략
1. **최소한의 정보로 간결하게 질문하기**
 - **Bad**: "인공지능이 인간을 대체할 가능성에 대해 논문 형식으로 3페이지 작성해줘."
 - **Good**: "AI가 인간의 직업을 대체할 가능성에 대한 요약을 제공해줘."
2. **Few-shot learning**: 예제 제공 → 모델이 학습 패턴을 유추하도록 유도.
3. **Chain of Thought (CoT) 활용 주의**
 - 추론 과정을 포함하면 성능이 저하될 수도 있음.

---

## 🔹 LLM 확장 기법

### 1️⃣ RAG (Retrieval-Augmented Generation)
- **LLM + 검색 기술 조합**
- 외부 데이터를 실시간으로 검색하여 최신 정보 기반으로 답변 생성.
- ✅ **실시간 업데이트 가능** → 최신 뉴스, 법률 정보 반영 가능.
- ❌ **정확한 검색 결과에 의존** → 검색 품질이 낮으면 결과도 부정확할 수 있음.

### 2️⃣ Fine-Tuning (파인튜닝)
- 특정 도메인(ex. 의료, 법률, 금융)에 맞게 **추가 훈련**하는 방법.
- ✅ **도메인 특화 성능 향상**
- ❌ **훈련 비용이 많이 들고, 새로운 데이터가 필요함**.

---

## 🔹 LLM이 거대한 이유?

### 🧮 부동소수점 (Floating Point, 실수) 사용
- 컴퓨터는 **무한한 실수를 표현할 수 없음** → 근사값(approximation) 사용.
- **문제점**:
- 계산량이 증가하여 **연산 비용이 커짐**.
- 모델 크기가 기하급수적으로 커짐.

### 🔢 해결책: **양자화 (Quantization)**
- 모델의 부동소수점을 낮은 정밀도로 변환하여 크기와 속도를 최적화.
- 예: π ≈ **3.141592…** → **3.14** 로 변환.
- ✅ **장점**
- 메모리 절약 (RAM, VRAM 사용 감소).
- 연산 속도 향상 → 배포 및 실시간 응답 최적화.

---

🚀 **최신 트렌드**
- **RAG** → 실시간 정보 검색
- **Fine-Tuning** → 도메인 최적화
- **Prompt Engineering** → 효율적인 질문 설계

